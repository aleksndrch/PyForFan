{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Подгрузка библиотек:\n",
    "import requests\n",
    "import re\n",
    "from lxml import html\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Основная часть программы работает напрямую, чтение ссылок из файла использовалось при отладке\n",
    "#Работаем с файлами (1) или напрямую (0):\n",
    "work_with_file = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Выдергиваем ссылки\n",
    "#Пишем их в список (url_lst) или в файл (links.txt), таблицу разбираем на строки и выбираем элементы выравненные влево (align:left)\n",
    "#так как таких элементов несколько, искомый элемент каждый третий (видно из структуры таблицы)\n",
    "\n",
    "url_lst=[]\n",
    "\n",
    "links_url = \"https://ru.wikipedia.org/wiki/%D0%A1%D0%BF%D0%B8%D1%81%D0%BE%D0%BA_%D0%B3%D0%BE%D1%80%D0%BE%D0%B4%D0%BE%D0%B2_%D0%A0%D0%BE%D1%81%D1%81%D0%B8%D0%B8\"\n",
    "r = requests.get(links_url).text\n",
    "\n",
    "url_soup = BeautifulSoup(r, \"lxml\")\n",
    "url_data = url_soup.find('table').find_all('td', {'align':'left'})\n",
    "\n",
    "for i in range(0,len(url_data),3):\n",
    "#Вывод ссылок в массив:\n",
    "    if work_with_file == 0:\n",
    "        url_lst.append(url_data[i].find('a').get('href'))\n",
    "#Вывод ссылок в файл:    \n",
    "    else:\n",
    "        with open('links.txt','a') as out_file:\n",
    "            out_file.write(url_data[i].find('a').get('href'))\n",
    "            out_file.write('\\n')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Поиск минимального года, можно запустить один раз, потому что долго\n",
    "reqtable = \"Численность населения\"\n",
    "min_year = 2017\n",
    "wiki_url = \"https://ru.wikipedia.org\"\n",
    "i=0\n",
    "#Пробежимся по всем страницам:\n",
    "for cities in range(0,len(url_lst),1):\n",
    "    \n",
    "    url = wiki_url+url_lst[cities]\n",
    "    r = requests.get(url).text\n",
    "\n",
    "#Так как у таблицы с численностью населения нет отдельного идентификатора, то ищем все таблицы (find_all)\n",
    "#и просматриваем их заголовки ('th'), на случай если у таблицы нет заголовка, вводим дополнительное условие (!=None)\n",
    "#для того чтобы избежать ошибки обращения .text к None, затем определяем наименьший год (find и re.match)\n",
    "#и сравниваем его с ранее найденным минимальным годом, для наглядности во время поиска выводится строка из '-'\n",
    "#Результат выполнения функции минимальный год (min_year) и количество пройденных ссылок (i)\n",
    "\n",
    "    min_soup = BeautifulSoup(r, \"lxml\")\n",
    "    min_datas = min_soup.find_all('table', {'class':\"standard\"})\n",
    "    \n",
    "    for min_data in min_datas:\n",
    "        mtable_name = min_data.find('th')\n",
    "        \n",
    "        if mtable_name != None and mtable_name.text == reqtable:\n",
    "            i=i+1\n",
    "            min_year_new = min_data.find('tr', {'class': 'bright'}).find('th').text\n",
    "            min_year_new = int(re.match('\\d\\d\\d\\d', min_year_new).group())\n",
    "            print('-', end = '')\n",
    "            \n",
    "            if min_year_new < min_year:\n",
    "                min_year = min_year_new\n",
    "            break\n",
    "            \n",
    "print(\"Минимальный год:\", min_year,',',\"Количество обработанных ссылок:\",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reqtable = \"Численность населения\"\n",
    "title =    \"city/year\"\n",
    "min_year    = 1500\n",
    "recent_year = 2017\n",
    "\n",
    "tab_counter = 0 #Cчетчик таблиц\n",
    "\n",
    "wiki_url = \"https://ru.wikipedia.org\"\n",
    "#year_lst=[]\n",
    "#data_lst=[]\n",
    "city_lst=[\"city/year\"]\n",
    "first_city    = 0 #В целях отладки (чтобы проще было разобраться с проблемными городами)\n",
    "num_of_cities = len(url_lst)\n",
    "num_of_years  = len([0 for i in range(min_year,recent_year+1,1)])\n",
    "#Сгенерируем список:\n",
    "result = [[0 for i in range(0,num_of_years,1)] for j in range(first_city,num_of_cities+1,1)]\n",
    "\n",
    "\n",
    "for i in range(len(result[0])):\n",
    "    result[0][i] = min_year+i\n",
    "\n",
    "\n",
    "#Дергаем значения:\n",
    "for cities in range(first_city,num_of_cities,1):\n",
    "    t=0\n",
    "    twintab_counter = 0\n",
    "    tab_counter     = 0 #Cчетчик таблиц\n",
    "    \n",
    "    year_lst=[]\n",
    "    data_lst=[]\n",
    "    \n",
    "    print('*', end = '')\n",
    "    \n",
    "    url = wiki_url+url_lst[cities]\n",
    "#   print(url)\n",
    "    r = requests.get(url).text\n",
    "    \n",
    "    soup = BeautifulSoup(r, \"lxml\")\n",
    "    datas = soup.find_all('table', {'class': [\"standard\",\"wikitable\"]})\n",
    "    \n",
    "    num_of_tables = len(datas) #Число таблиц на странице\n",
    "    \n",
    "    city = soup.find('h1', {'class':\"firstHeading\"}).text \n",
    "    \n",
    "#А вот нет у нас таблиц (datas = None и в цикл мы соответсвенно не войдем):  \n",
    "\n",
    "    if datas == []:\n",
    "        city_lst.append(city+\" (Нет данных)\") #Воизбежание повторов\n",
    "        for column in range(0,num_of_years,1):\n",
    "                    result[cities-first_city+1][column] = None\n",
    "\n",
    "    for data in datas: \n",
    "        \n",
    "        mtable_name = data.find('th')\n",
    "\n",
    "        if mtable_name != None and mtable_name.text == reqtable and twintab_counter == 0:\n",
    "\n",
    "#ГОРОДА:\n",
    "            city_lst.append(city) #Воизбежание повторов\n",
    "#ГОДА:\n",
    "            items = data.find_all('tr', {'class': 'bright'})\n",
    "            for item in items:\n",
    "                years = item.find_all('th')\n",
    "                for year in years:\n",
    "                    output = year.text\n",
    "                    output = re.match('\\d\\d\\d\\d', output)\n",
    "                    \n",
    "                    if output != None:\n",
    "                        year_lst.append(int(output.group()))\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "#ЦИФРЫ:                        \n",
    "            items = data.find_all('tr', {'align':'center'})\n",
    "            for item in items:\n",
    "                peoples = item.find_all('td')\n",
    "                for people in peoples:\n",
    "                    output = people.text\n",
    "                    output = re.sub(r'\\D|\\s','', output)\n",
    "                    \n",
    "                    if output != '':\n",
    "                        data_lst.append(int(output))\n",
    "                    else:\n",
    "                        break\n",
    "#            print(data_lst)               \n",
    "#Заносим строки:\n",
    "            for column in range(0,num_of_years,1):\n",
    "                if t < len(data_lst) and result[0][column] == year_lst[t]:\n",
    "                    result[cities-first_city+1][column] = data_lst[t]\n",
    "                    t=t+1\n",
    "                else:\n",
    "                    result[cities-first_city+1][column] = None\n",
    "             \n",
    "            twintab_counter = 1\n",
    "            \n",
    "#Если таблицы с населением нет:\n",
    "        else:\n",
    "            if tab_counter == num_of_tables-1 or datas == None:\n",
    "                city_lst.append(city+\" (Нет данных)\") #Воизбежание повторов\n",
    "                for column in range(0,num_of_years,1):\n",
    "                    result[cities-first_city+1][column] = None\n",
    "            else:\n",
    "                tab_counter = tab_counter+1          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Запись в файл:\n",
    "with open(\"output.txt\",'w') as ouf:\n",
    "    \n",
    "    for cities in range(0, num_of_cities-first_city+1, 1):\n",
    "        ouf.write(str(city_lst[cities]))\n",
    "        ouf.write('\\t')\n",
    "        for years in range(0, num_of_years,1):\n",
    "            ouf.write(str(result[cities][years]))\n",
    "            ouf.write('\\t')         \n",
    "        ouf.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
